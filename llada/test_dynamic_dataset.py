import json
import tempfile
import os
from transformers import AutoTokenizer
from finetune_dynamic_length import DynamicLengthDataset, DynamicLengthConfig, ENLARGE_TOKENS

test_list = [1, 3, 5, 7, 9]

def test_dataset_creation():
    """æµ‹è¯•æ•°æ®é›†åˆ›å»º"""
    print("1. æµ‹è¯•æ•°æ®é›†åˆ›å»º...")
    
    # ä½¿ç”¨GSM8Kæ•°æ®é›†
    data_file = "dataset/gsm8k_train.jsonl"
    print(f"ä½¿ç”¨GSM8Kæ•°æ®é›†: {data_file}")


    try:
        # åˆå§‹åŒ–tokenizer
        print("2. åŠ è½½tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained(
            "GSAI-ML/LLaDA-8B-Instruct",
            trust_remote_code=True,
            padding_side="right"
        )
        
        # æ·»åŠ ç‰¹æ®Štokens
        special_tokens = list(ENLARGE_TOKENS.values())
        existing_tokens = set(tokenizer.get_vocab().keys())
        new_tokens = [token for token in special_tokens if token not in existing_tokens]
        
        if new_tokens:
            print(f"3. æ·»åŠ æ–°çš„ç‰¹æ®Štokens: {new_tokens}")
            tokenizer.add_tokens(new_tokens)
        
        # åˆ›å»ºé…ç½®
        config = DynamicLengthConfig(
            initial_length=128,
            max_length=2048,
            max_expansions=5
        )
        
        # åˆ›å»ºæ•°æ®é›†
        print("4. åˆ›å»ºæ•°æ®é›†...")
        dataset = DynamicLengthDataset(
            data_path=data_file,
            tokenizer=tokenizer,
            config=config
        )

        print(f"âœ… æ•°æ®é›†åˆ›å»ºæˆåŠŸï¼å…± {len(dataset)} ä¸ªæ ·æœ¬")
        return dataset, tokenizer

    except Exception as e:
        print(f"âŒ æ•°æ®é›†åˆ›å»ºå¤±è´¥: {e}")
        raise

def test_special_token_insertion(dataset, tokenizer):
    """æµ‹è¯•ç‰¹æ®Štokenæ’å…¥é€»è¾‘"""
    print("\n5. æµ‹è¯•ç‰¹æ®Štokenæ’å…¥é€»è¾‘...")
    
    for i, example in enumerate(dataset.examples):
        if i in test_list:
            print(f"\n--- æ ·æœ¬ {i+1} ---")
            print(f"åŸå§‹å›ç­”é•¿åº¦: {example['response_length']} tokens")
            print(f"ä½¿ç”¨çš„ç‰¹æ®Štoken: {example['enlarge_token']}")
            
            # æ£€æŸ¥åŸå§‹å›ç­”
            original_tokens = tokenizer.encode(example['original_response'], add_special_tokens=False)
            print(f"åŸå§‹å›ç­”å®é™…tokenæ•°: {len(original_tokens)}")
            
            # æ£€æŸ¥ä¿®æ”¹åçš„å›ç­”
            modified_tokens = tokenizer.encode(example['response'], add_special_tokens=False)
            print(f"ä¿®æ”¹åå›ç­”tokenæ•°: {len(modified_tokens)}")
            
            # æ£€æŸ¥ç‰¹æ®Štokenæ˜¯å¦æ­£ç¡®æ’å…¥
            enlarge_token_id = tokenizer.convert_tokens_to_ids(ENLARGE_TOKENS[example['enlarge_token']])
            if enlarge_token_id in modified_tokens:
                positions = [j for j, token_id in enumerate(modified_tokens) if token_id == enlarge_token_id]
                print(f"ç‰¹æ®Štokenä½ç½®: {positions}")
            else:
                print("ç‰¹æ®Štokenæœªæ‰¾åˆ°")
            
            # æ˜¾ç¤ºéƒ¨åˆ†å†…å®¹
            print(f"åŸå§‹å›ç­”: {example['original_response']}")
            print(f"ä¿®æ”¹åå›ç­”: {example['response']}")
            
            # éªŒè¯é€»è¾‘
            if example['response_length'] <= 128:
                assert example['enlarge_token'] == 'enough', f"çŸ­å›ç­”åº”è¯¥ä½¿ç”¨enough tokenï¼Œä½†ä½¿ç”¨äº†{example['enlarge_token']}"
                assert example['response'].endswith(ENLARGE_TOKENS['enough']), "çŸ­å›ç­”åº”è¯¥åœ¨æœ«å°¾æ·»åŠ enough token"
            else:
                assert example['enlarge_token'] == 'enlarge', f"é•¿å›ç­”åº”è¯¥ä½¿ç”¨enlarge tokenï¼Œä½†ä½¿ç”¨äº†{example['enlarge_token']}"
                assert ENLARGE_TOKENS['enlarge'] in example['response'], "é•¿å›ç­”åº”è¯¥åŒ…å«enlarge token"

def test_dataset_getitem(dataset, tokenizer):
    """æµ‹è¯•æ•°æ®é›†çš„__getitem__æ–¹æ³•"""
    print("\n6. æµ‹è¯•æ•°æ®é›†__getitem__æ–¹æ³•...")
    
    for i in test_list:  # æµ‹è¯•å‰3ä¸ªæ ·æœ¬
        print(f"\n--- æµ‹è¯•æ ·æœ¬ {i+1} ---")
        
        item = dataset[i]
        
        # æ£€æŸ¥è¿”å›çš„å­—æ®µ
        expected_keys = [
            'input_ids', 'attention_mask', 'labels', 'prompt_length',
            'enlarge_token_id', 'response_length', 'enlarge_token',
            'length_category', 'prompt', 'response', 'original_response'
        ]
        
        for key in expected_keys:
            assert key in item, f"ç¼ºå°‘å­—æ®µ: {key}"
        
        print(f"input_idsé•¿åº¦: {len(item['input_ids'])}")
        print(f"prompté•¿åº¦: {item['prompt_length']}")
        print(f"responseé•¿åº¦: {item['response_length']}")
        print(f"enlarge_token_id: {item['enlarge_token_id']}")
        print(f"enlarge_token: {item['enlarge_token']}")
        
        # éªŒè¯labelsçš„è®¾ç½®ï¼ˆpromptéƒ¨åˆ†åº”è¯¥æ˜¯-100ï¼‰
        labels = item['labels']
        prompt_length = item['prompt_length']
        
        # æ£€æŸ¥promptéƒ¨åˆ†çš„labelsæ˜¯å¦ä¸º-100
        prompt_labels = labels[:prompt_length]
        assert all(label == -100 for label in prompt_labels), "Promptéƒ¨åˆ†çš„labelsåº”è¯¥å…¨éƒ¨ä¸º-100"
        
        # æ£€æŸ¥responseéƒ¨åˆ†æ˜¯å¦æœ‰æœ‰æ•ˆçš„labels
        response_labels = labels[prompt_length:]
        valid_labels = [label for label in response_labels if label != -100]
        print(f"æœ‰æ•ˆlabelsæ•°é‡: {len(valid_labels)}")
        
        print("âœ… æ ·æœ¬éªŒè¯é€šè¿‡")


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ å¼€å§‹æµ‹è¯•DynamicLengthDataset...")
    
    try:
        # æµ‹è¯•æ•°æ®é›†åˆ›å»º
        dataset, tokenizer = test_dataset_creation()
        
        # æµ‹è¯•ç‰¹æ®Štokenæ’å…¥
        test_special_token_insertion(dataset, tokenizer)
        
        # æµ‹è¯•__getitem__æ–¹æ³•
        test_dataset_getitem(dataset, tokenizer)
        
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼DynamicLengthDatasetå·¥ä½œæ­£å¸¸ã€‚")
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
